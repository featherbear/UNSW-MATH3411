<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>Lecture Nine - MATH3411 Musings</title><meta name=renderer content=webkit><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=cache-control content=no-transform><meta http-equiv=cache-control content=no-siteapp><meta name=theme-color content=#f8f5ec><meta name=msapplication-navbutton-color content=#f8f5ec><meta name=apple-mobile-web-app-capable content=yes><meta name=apple-mobile-web-app-status-bar-style content=#f8f5ec><meta name=author content=z5206677><meta name=description content="Chapter Four - Information Theory Let $I(p)$ be the amount of information given by an event of probability $p$."><meta name=keywords content=Hugo,theme,even><meta name=generator content="Hugo 0.58.2 with theme even"><link rel=canonical href=../../post/lec09/><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../manifest.json><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><link href=../../dist/even.c2a46f00.min.css rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin=anonymous><link rel=stylesheet href=../../css/typedjs.shortcode.css><meta property=og:title content="Lecture Nine"><meta property=og:description content="Chapter Four - Information Theory Let $I(p)$ be the amount of information given by an event of probability $p$."><meta property=og:type content=article><meta property=og:url content=/post/lec09/><meta property=article:published_time content=2019-10-15T12:06:28+11:00><meta property=article:modified_time content=2019-10-15T12:06:28+11:00><meta itemprop=name content="Lecture Nine"><meta itemprop=description content="Chapter Four - Information Theory Let $I(p)$ be the amount of information given by an event of probability $p$."><meta itemprop=datePublished content=2019-10-15T12:06:28&#43;11:00><meta itemprop=dateModified content=2019-10-15T12:06:28&#43;11:00><meta itemprop=wordCount content=423><meta itemprop=keywords content><meta name=twitter:card content=summary><meta name=twitter:title content="Lecture Nine"><meta name=twitter:description content="Chapter Four - Information Theory Let $I(p)$ be the amount of information given by an event of probability $p$."><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=../../ class=logo>MATH3411 Musings</a></div><div class=mobile-navbar-icon><span></span><span></span><span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><a href=../../><li class=mobile-menu-item>Home</li></a><a href=https://github.com/featherbear/UNSW-MATH3411><li class=mobile-menu-item>GitHub</li></a><a href=../../categories/><li class=mobile-menu-item>Categories</li></a></ul></nav><div class=container id=mobile-panel><header id=header class=header><div class=logo-wrapper><a href=../../ class=logo>MATH3411 Musings</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=../../>Home</a></li><li class=menu-item><a class=menu-item-link href=https://github.com/featherbear/UNSW-MATH3411>GitHub</a></li><li class=menu-item><a class=menu-item-link href=../../categories/>Categories</a></li></ul></nav></header><main id=main class=main><div class=content-wrapper><div id=content class=content><article class=post><header class=post-header><h1 class=post-title>Lecture Nine</h1><div class=post-meta><span class=post-time>2019-10-15</span></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>Contents</h2><div class="post-toc-content always-active"><nav id=TableOfContents><ul><li><a href=#chapter-four-information-theory>Chapter Four - Information Theory</a><ul><li><a href=#shannon-entropy-of-s>Shannon entropy of S</a></li><li><a href=#gibbs-inequality>Gibbs&rsquo; Inequality</a></li><li><a href=#maximum-entropy-theorem>Maximum Entropy Theorem</a></li><li><a href=#first-source-coding-theorem>First Source-Coding Theorem</a></li><li><a href=#shannon-fano-coding>Shannon-Fano Coding</a></li></ul></li></ul></nav></div></div><div class=post-content><h1 id=chapter-four-information-theory>Chapter Four - Information Theory</h1><p>Let $I(p)$ be the amount of information given by an event of probability $p$.</p><ul><li>$I(p) \ge 0$ for $0 \le p \le 1$</li><li>$I(1) = 0$</li><li>$I$ is continuous</li><li>$I(p_1p_2)=I(p_1) + I(p_2)$ for independent events</li></ul><blockquote><p>$I(p) = kln(p)$ for some $k \lt 0$</p></blockquote><ul><li>$I(p) = I_r(p) = -log_r p$<ul><li>MATH3411 always uses $p=2$</li></ul></li></ul><h2 id=shannon-entropy-of-s>Shannon entropy of S</h2><p>$H(S) = H<em>2(S) = \sum</em>{i=1}^q p_iI(p<em>i) = - \sum</em>{i=1}^q p_i log_2 p_i$</p><p><strong>Example - Single coin toss</strong><br>H(S) = -<sup>1</sup>&frasl;<sub>2</sub> log_2 <sup>1</sup>&frasl;<sub>2</sub> + -<sup>1</sup>&frasl;<sub>2</sub> log_2 <sup>1</sup>&frasl;<sub>2</sub></p><p><strong>Example - Double coin toss, counting heads</strong><br>p_1 = P(TT) = <sup>1</sup>&frasl;<sub>4</sub><br>p_2 = P(HT, TH) = <sup>2</sup>&frasl;<sub>4</sub><br>p_3 = P(HH) = <sup>1</sup>&frasl;<sub>4</sub></p><p>There are 0 heads <sup>1</sup>&frasl;<sub>4</sub> of the time.<br>There are 1 heads <sup>1</sup>&frasl;<sub>2</sub> of the time.<br>There are 2 heads <sup>1</sup>&frasl;<sub>4</sub> of the time.</p><p>H(S) = (-<sup>1</sup>&frasl;<sub>4</sub> log_2 <sup>1</sup>&frasl;<sub>4</sub>) + (-<sup>1</sup>&frasl;<sub>2</sub> log_2 <sup>1</sup>&frasl;<sub>2</sub>) + (-<sup>1</sup>&frasl;<sub>4</sub> log_2 <sup>1</sup>&frasl;<sub>4</sub>) = <sup>3</sup>&frasl;<sub>2</sub></p><p>On average, there are <sup>3</sup>&frasl;<sub>2</sub> bits of information</p><p><strong>Example - Huffman code</strong><br>$P = 0.3, 0.2, 0.2, 0.1, 0.1, 0.1$<br>$H(S) = -\sum_{i=1}^q p_ilog_2p_i$
= 2.446</p><p><strong>Example - Arithmetic coding</strong><br>$P = 0.4, 0.2, 0.2, 0.1, 0.1$<br>H(S) ~= 0.639 digits/symbol</p><h2 id=gibbs-inequality>Gibbs&rsquo; Inequality</h2><p>If p_1, &hellip;, p_q and p&rsquo;1, &hellip;, p&rsquo;q are probability distributions then</p><p>-\sum_{i=1}^q p_1 log_r p<em>i \le -\sum</em>{i=1}^q p_1 log_r p&rsquo;i</p><p>And</p><p>\sum_{i=1}^q p_1 log_r p&rsquo;i/p_1 \le 0</p><p>There is an equality if and only if pi = p&rsquo;i for all i</p><h2 id=maximum-entropy-theorem>Maximum Entropy Theorem</h2><p>For any source S with q symbols</p><p>H_r(S) \le log_r q</p><p>with equality iff all symbols are equally likely ( $H_r(S) = log_r |S|$ )</p><h2 id=first-source-coding-theorem>First Source-Coding Theorem</h2><p>For each radix $r$ UD-code $C$ for source $S$, $H_r(S) \le L_r$,
with equality iff $p_i = r^{-l_i} for all $i$ and $K<em>r = \sum</em>{i=1}^q r^{-l_i} = 1$</p><p>-&gt; K = \sum_{i=1}^q 2^{-l_i} = 1</p><p>// TODO: PINK ELEPHANT</p><h2 id=shannon-fano-coding>Shannon-Fano Coding</h2><p>Input: Soruce S = {s_1, &hellip;, s_q} probabilities p_1, &hellip;, p_q
Output: radix r I-code C for S given a decision key</p><p>Algorithm::</p><ul><li>Define the codeword lengths l_i = ceil(-log_r(p_i))</li><li>Construct the standard I-code for -l_1, &hellip;, l_q</li></ul><p>&ndash;
Example
r = 2</p><p>Find the probabilities
find the reciprocals (1/p)
Find the smallest power of r, which the reciprocal is smaller than</p><p>They give us the lengths of the tree</p><ul><li>Unlike the huffman encoding scheme, we can quickly know the codeword lengths</li></ul><p>H<em>r(S) \le L</em>{r,H} \le L_{r,SF} \lt H_r(S) + 1</p><p>Huffman - better compression
SF can be used even when only some p_i are known
SF lengths prior to encoding</p><p>Worst by at most one.</p></div><footer class=post-footer><nav class=post-nav><a class=prev href=../../post/lec08/><i class="iconfont icon-left"></i><span class="prev-text nav-default">Lecture 08</span>
<span class="prev-text nav-mobile">Prev</span></a>
<a class=next href=../../post/lec10/><span class="next-text nav-default">Lecture 10</span>
<span class="next-text nav-mobile">Next</span>
<i class="iconfont icon-right"></i></a></nav></footer></article></div></div></main><footer id=footer class=footer><div class=social-links><a href=mailto:z5206677@student.unsw.edu.au class="iconfont icon-email" title=email></a><a href=https://www.linkedin.com/in/andrewjinmengwong/ class="iconfont icon-linkedin" title=linkedin></a><a href=https://github.com/featherbear class="iconfont icon-github" title=github></a><a href=https://www.instagram.com/_andrewjwong/ class="iconfont icon-instagram" title=instagram></a><a href=../../index.xml type=application/rss+xml class="iconfont icon-rss" title=rss></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a></span>
<span class=division>|</span>
<span class=theme-info>Theme -
<a class=theme-link href=https://github.com/olOwOlo/hugo-theme-even>Even</a></span>
<span class=copyright-year>&copy;
2019
<span class=heart><i class="iconfont icon-heart"></i></span><span class=author>Andrew Wong (z5206677)</span></span></div></footer><div class=back-to-top id=back-to-top><i class="iconfont icon-up"></i></div></div><script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin=anonymous></script><script type=text/javascript src=../../dist/even.26188efa.min.js></script><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$'],['\\(','\\)']]},showProcessingMessages:false,messageStyle:'none'};</script><script async src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin=anonymous></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-107434487-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script src=../../js/typed.js@2.0.9></script><script src=../../js/typedjs.shortcode.js></script></body></html>